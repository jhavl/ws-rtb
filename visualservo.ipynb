{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image-based visual servoing\n",
    "\n",
    "We will create a classical image-based visual servoing system, which will drive the camera (mounted on the end of a Franka-Emika Panda robot) toward a goal comprising four 3D points.\n",
    "\n",
    "Some references for this material are:\n",
    "\n",
    "* [Visual servo control, Part I: Basic approaches, François Chaumette, S. Hutchinson](https://hal.inria.fr/inria-00350283/document)\n",
    "* Robotics, Vision & Control, §15, Peter Corke, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring CoLab\n",
    "\n",
    "To make this run nicely in CoLab we have to install a bunch of packages.  This will be slow and at the end you will need to restart the kernel.  Push the `RESTART KERNEL` button that appears or from the menu above `Runtime/RestartRuntime`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy --upgrade\n",
    "!pip install roboticstoolbox-python\n",
    "!pip install -U ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can import some of the packages we need throughout, NumPy, and configure the CoLab environment a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# display result of assignments\n",
    "%config ZMQInteractiveShell.ast_node_interactivity = 'last_expr_or_assign'\n",
    "# make NumPy display a bit nicer\n",
    "np.set_printoptions(linewidth=100, formatter={'float': lambda x: f\"{x:10.4g}\" if abs(x) > 1e-10 else f\"{0:10.4g}\"})\n",
    "# make cells nice and wide\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(data=\"\"\"\n",
    "<style>\n",
    "    div#notebook-container    { width: 95%; }\n",
    "    div#menubar-container     { width: 65%; }\n",
    "    div#maintoolbar-container { width: 99%; }\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now import the classes we need from the Robotics, Machine Vision, and SpatialMaths Toolboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from roboticstoolbox import models\n",
    "from machinevisiontoolbox import CentralCamera, mkgrid\n",
    "from spatialmath import SE3\n",
    "from spatialmath.base import plotvol3, plot_sphere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a 7-axis Franka-Emika Panda robot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot = models.DH.Panda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and a perspective camera with default parameters, which will be listed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = CentralCamera.Default()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which we see is a megapixel (1000x1000) image with an 8mm focal length lens.  Its field of view is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.fov() * np.rad2deg(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "of the order of 60°."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will place four objects in the world, in a vertical plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = mkgrid(side=0.4, n=2, pose=SE3(2, 0, 0.6) * SE3.Ry(90, unit='deg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mkgrid` creates a 2x2 grid of 3D coordinates within a 0.4x0.4 area in the xy-plane.  The `pose` argument rotates that grid about the y-axis and translates it 2m out along the x-axis.  The result is four 3D points, one per column.\n",
    "\n",
    "To visualize these is easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotvol3([-1, 4, -2.5, 2.5, -1, 4])\n",
    "plot_sphere(0.05, P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The projection of these points for camera at the origin, looking along the x-axis is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = camera.project_point(P, pose=SE3.Ry(90, unit='deg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and these are the coordinates of the 3D points on the image plane with units of pixels.  Again, points correspond to columns in the array.\n",
    "\n",
    "We can visualize this on the `camera` object's virtual image plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.plot_point(p);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's say that the desired image-plane coordinates are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pstar = mkgrid(n=2, side=700)[:2, :] + 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error between the current image plane coordinates and the desired is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = pstar - p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which we can reshape as a 1D array in column order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = e.flatten(order=\"F\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image Jacobian, or interaction matrix is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Jv = camera.visjac_p(p, depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't go into all the details of image-based visual servong here, but the Jacobian has a dependency on the depth of the points, their distance from the camera parallel to the optical axis.  In this case we have given a value of 2, but in practice this needs to be determined in an application specific way, or estimated online.\n",
    "\n",
    "The camera velocity required to move toward the desired view is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = 0.01 * np.linalg.pinv(Jv) @ e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where we have added a scalar gain term.  We see is mostly velocity in the camera's x- and z-directions, toward the target.\n",
    "\n",
    "The required robot joint velocity can be determined from the manipulator Jacobian, but first we need to know the joint configuration to place the camera at this pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iksol = robot.ikine_LMS(camera.pose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which was successful. Now we can compute the manipulator Jacobian, in the end-effector frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = robot.jacobe(iksol.q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then using resolved-rate motion control, to transform the desired camera velocity (in the camera frame) to joint velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qd = np.linalg.pinv(J) @ v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the \"guts\" of an image-basd visual servoing system.  To make it complete we need to put the above lines into an integration loop.\n",
    "\n",
    "If you manage to get that done (congratulations!) then other things to investigate would be:\n",
    "\n",
    "* animate the robot using Swift\n",
    "  * you could even add the small spheres into the Swift environment, using the `spatialgeometry` package, talk to us about this\n",
    "* animate the display of points on the image plane\n",
    "* experiment with different values of gain\n",
    "* experiment with different initial camera poses (joint configurations) or target configurations\n",
    "* experiment with different values of depth when computing the visual Jacobian.  If you're feeling ambitious, perhaps create a depth estimator."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b7d6b0d76025b9176285a6442c3dd6dd39bcfe7241029b7898b7106bd5e9b472"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('dev': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
